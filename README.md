# word2vecで色々やってみる

# 2019/04
## word2vec
### 実行手順
- sourceディレクトリ内にテキストデータorソースコードなどが入ったディレクトリを用意します
```
word2vec/source/hoge/***.txt
word2vec/source/hoge/***.class
```
- mainで実行するか個別で実行します
```
$ python main.py
# source内のディレクトリを指定
# 類似度検索をしたい単語を入力
```
### 個別実行
- merge_source.pyでsourceディレクトリ内のソースorテキストをまとめる
```
$ python merge_source.py
#source内のディレクトリを指定
```
- textディレクトリ内にまとめられたテキストファイルが生成される
- ソースコードなら`replace.py`をテキストデータなら`mecab.py`を実行
```
$ python replace.py
```
- 対応ソースコードをコーパスとしてモデリング
```
$ python model.py
#ソースコードが入っているsource内のディレクトリを指定
# optionを適宜設定してモデリングを実行
```
- 生成されたモデルから類似
```
$ python result.py
```
- プロット
```
$ python plot.py
```

## その他
- miningディレクト内は雑な実装が多いので気をつけてください
- 今後直します

## model.pyのオプション
### sentences
解析に使う1行1センテンスで書かれた文書。日本語の場合はLineSentenceフォーマットを使えばうまくいった。単語が空白文字で区切られていて、文章は改行で区切られていれば問題ない。
### sg
{1,0}の整数で訓練アルゴリズムを設定できる。 1を選べばskip-gramで、0ならばCBOWを使う。
### size
特徴ベクトルの次元を設定する。
### window
文書内における現在の単語と予測した単語の間の距離の最大値を設定する。言い換えると、文脈の最大単語数を設定する。
### alpha
学習率の初期値を設定する。
### min_alpha
訓練の過程で徐々に落ちていく学習率の最小値を設定する。
### seed
乱数を生成する際のシード番号を設定する。
### min_count
一定の頻度以下の単語を除外する際の値を設定する。
### max_vocab_size
語彙ベクトルを構築している際のメモリ制限を設定する。
### sample
(0, 1e-5)の範囲で、頻度語がランダムに削除される閾値を設定する。高速化と精度向上を狙っており、自然言語処理においても高頻度語はストップワードとして除去するなどの対応が取られている。
### workers
モデルを訓練するために多くのワーカースレッドを利用するかどうか設定する。（並列化関連）
### hs
{1,0}の整数で、1であれば階層的ソフトマックスがモデルの訓練で用いられ、0であり引数negativeがnon-zeroであればネガティヴサンプリングが設定できる。全部計算することが大変なので、階層的なグループに分けて各グループごとに学習するというのがモチベーション。
### negative
0よりも大きければネガティブサンプリングが用いられる。5〜20などを選び、どれだけノイズワードが描かれているかを識別する。0であればネガティブサンプリングが適用されない。ネガティブサンプリングは計算高速化を目的に出力層で正解ニューロン以外のニューロンを更新しないように学習する手法。
### cbow_mean
{1,0}の整数で、0であれば単語ベクトルの合計を用い、1であればCBOWが用いられた際の平均が用いられる。
### hashfxn
訓練の再現性のためにランダムに初期値のウエイト付けできる。
### iter
コーパスにおける繰り返し回数（エポック数）を設定できる。
### trim_rule
ある単語を語彙に含めるべきかどうかを識別する、語彙のトリミングルールを設定する。
### sorted_vocab
{1,0}の整数で、1であれば頻度の降順で語彙を並べ替える。
### batch_words
ワーカースレッドにわたすバッチの大きさを指定する。（並列化関連）
### compute_loss
Trueであれば損失関数の計算と蓄積を行う。
### callbacks
訓練時の特定の段階で実行する際に必要なコールバックのリストを指定できる。
